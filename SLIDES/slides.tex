\documentclass{beamer}
\mode<presentation>
\usepackage[utf8]{inputenc}
%\usepackage{listings}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{graphics}
\uselanguage{french}
\usetheme{Warsaw}
\usecolortheme{orchid}

\title{Apprentissage par Renforcement Hors Ligne}
\subtitle{(BATCH)}
\date{15 décembre 2015}
\author{Alexandre Gerussi, Léo Pérard, Lucas Seguinot}
\institute{M2 MOCAD - IIR}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Plan de l'exposé}
\tableofcontents
\end{frame}

\section{Introduction}
\frame{
\frametitle{Pourquoi batch ?} % LUCAS
\begin{itemize}
\item Algorithmes en ligne : interactions libres avec l'environnement
  \bigskip
\item Pas toujours possible :
\begin{itemize}
\item Sondages
\item Conduite de vélo, bras robotique... Nécessite un opérateur humain
\end{itemize}
\end{itemize}
}
\frame{
\frametitle{Types de batch} % LEO
\begin{center}
\includegraphics<1>[width=8cm]{../IMG/schema-batch.pdf}%
\end{center}
}

\section{Principes généraux} % ALEXANDRE
\frame{
\frametitle{Principes généraux}
\begin{itemize}
\item nombreux algorithmes, différant essentiellement sur des détails
\item itérations dans SARSA ou Q-learning: exploration + convergence
\item $\Rightarrow$ {\bf experience replay}: faire converger sans explorer
\item itérations en-ligne: locales, propagation aléatoire par les itérations
\item $\Rightarrow $ {\bf fitting}: accélérer et stabiliser la propagation en globalisant les MAJ
\end{itemize}
}

\section{Kernel Based Approximate Dynamic Programming} % LEO
\frame{
\frametitle{Kernel Based Approximate Dynamic Programming}
\begin{block}{Phase 1}
\[
\hat{Q}_a^{i+1}(\sigma) = \sum_{(s,a,r,s') \in F_a} k(s,\sigma) [r + \gamma \hat{V}^i(s')]
\]
\end{block}
\begin{block}{Phase 2}
\[
\hat{V}^{i+1}(s) = \max_{a \in A} \hat{Q}_a^{i+1}(s)
\]
\end{block}
\begin{itemize}
	\item $F_a$ etant l'ensemble des transitions effectue via $a$
	\item $k$ defini une distance (weighted kernel) entre 2 etats
\end{itemize}
}

\frame{
    \frametitle{Application: Choix du portfolio optimal}
\begin{itemize}
	\item $s_t$ : valeur de l'action a un instant $t$
	\item $a_t$ : action d'investissement
\end{itemize}
\begin{block}{Maximisation}
\[
	W_{t+1} = (1 + a_t \frac{s_{t+1} - s_t}{s_t})W_t
\]
\end{block}
}

\section{Fitted Q-iteration} % LUCAS
\frame{
  \frametitle{Fitted Q-iteration}
  \begin{block}{Itération sur la valeur}
\[
Q^{i+1}(s,a) = \sum_{s'\in S} \mathcal{T}_{ss'}^a(\mathcal{R}_{ss'}^a + \gamma \max_{a'\in A} Q^i(s',a'))
\]
\end{block}
\begin{itemize}
\item $\mathcal{F}$ ensemble de transitions $(s,a,r,s')$
\item Base de donnée d'apprentissage $P$ :
  \[
  (s,a) \to r+\gamma\max_{a' \in A}\hat{Q}^i(s',a')
  \]
  \item Apprentissage supervisé $\to \hat{Q}^{i+1}$
\end{itemize}
}

\section{Least-Squares Policy Iteration} % ALEXANDRE
\frame{
\frametitle{Least-Squares Policy Iteration}
$$Q=K(Q) \longleftrightarrow Q= K'(Q) = \mathcal{P}_{\mathcal{Q}}^\perp(K(Q))$$
\begin{centering}
\includegraphics<1>[width=7cm]{../IMG/LS-policy-iteration.pdf}%
\end{centering}
}

\frame{
    \frametitle{Application: équilibre et conduite d'un vélo}
\begin{itemize}
\item rester debout et atteindre un but en vélo
\item valeurs sous contrôle:  
\begin{itemize}
\item force rotatoire à appliquer au guidon
\item placement du centre de masse par rapport au vélo
\end{itemize}
\item pûrement hors-ligne à partir de quelques milliers de trajectoires effectuées aléatoirement
\item experience replay: quelques passes de l'ensemble des données font converger
\end{itemize}
}

\end{document}
