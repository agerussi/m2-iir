\documentclass{beamer}
\mode<presentation>
\usepackage[utf8]{inputenc}
%\usepackage{listings}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{graphics}
\uselanguage{french}
\usetheme{Warsaw}
\usecolortheme{orchid}

\title{Apprentissage par Renforcement Hors Ligne}
\subtitle{(BATCH)}
\date{15 décembre 2015}
\author{Alexandre Gerussi, Léo Pérard, Lucas Seguinot}
\institute{M2 MOCAD - IIR}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Plan de l'exposé}
\tableofcontents
\end{frame}

\section{Introduction}
\frame{
\frametitle{Pourquoi batch ?} % LUCAS
\begin{itemize}
\item en ligne: interactions libres, voir illimitées avec l'environnement
\item pas toujours possible
\begin{itemize}
    \item sondages
    \item "conduite de vélo": nécessite un opérateur humain
    \item ?? : casse du matériel en cas d'échec
\end{itemize}
\end{itemize}
}
\frame{
\frametitle{Types de batch} % LEO
\begin{itemize}
\item pure batch 
\item growing batch
\item semi-batch
\end{itemize}
}

\section{Principes généraux} % ALEXANDRE
\frame{
\frametitle{Principes généraux}
\begin{itemize}
\item nombreux algorithmes, différant essentiellement sur des détails
\item itérations dans SARSA ou Q-learning: exploration + convergence
\item $\Rightarrow$ {\bf experience replay}: faire converger sans explorer
\item itérations en-ligne: locales, propagation aléatoire par les itérations
\item $\Rightarrow $ {\bf fitting}: accélérer et stabiliser la propagation en globalisant les MAJ
\end{itemize}
}

\section{Kernel Based Approximate Dynamic Programming} % LEO
\frame{
\frametitle{Kernel Based Approximate Dynamic Programming}
\begin{itemize}
\item  ??
\end{itemize}
}

\section{Fitted Q-iteration} % LUCAS
\frame{
  \frametitle{Fitted Q-iteration}
  \begin{block}{Itération sur la valeur}
\[
Q^{i+1}(s,a) = \sum_{s'\in S} \mathcal{T}_{ss'}^a(\mathcal{R}_{ss'}^a + \gamma \max_{a'\in A} Q^i(s',a'))
\]
\end{block}
\begin{itemize}
\item $\mathcal{F}$ ensemble de transitions $(s,a,r,s')$
\item Base de donnée d'apprentissage $P$ :
  \[
  (s,a) \to r+\gamma\max_{a' \in A}\hat{Q}^i(s',a')
  \]
  \item Apprentissage supervisé $\to \hat{Q}^{i+1}$
\end{itemize}
}

\section{Least-Squares Policy Iteration} % ALEXANDRE
\frame{
\frametitle{Least-Squares Policy Iteration}
$$Q=K(Q) \longleftrightarrow Q= K'(Q) = \mathcal{P}_{\mathcal{Q}}^\perp(K(Q))$$
\begin{centering}
\includegraphics<1>[width=7cm]{../IMG/LS-policy-iteration.pdf}%
\end{centering}
}

\frame{
    \frametitle{Application: équilibre et conduite d'un vélo}
\begin{itemize}
\item rester debout et atteindre un but en vélo
\item valeurs sous contrôle:  
\begin{itemize}
\item force rotatoire à appliquer au guidon
\item placement du centre de masse par rapport au vélo
\end{itemize}
\item pûrement hors-ligne à partir de quelques milliers de trajectoires effectuées aléatoirement
\item experience replay: quelques passes de l'ensemble des données font converger
\end{itemize}
}

\end{document}
